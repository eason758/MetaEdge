{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import entr\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log2, log\n",
    "from scipy.spatial import cKDTree\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDataset(data, p, test_set_type, SAR):\n",
    "    if test_set_type == 1:\n",
    "        print('Verify on random sample')\n",
    "        data = shuffle(data, random_state=0)\n",
    "    else:\n",
    "        print('Verify on time series')\n",
    "        \n",
    "    print('shape of data:', data.shape)\n",
    "    #data = data.reset_index(drop = True)\n",
    "    TO_SAR = data[data[SAR]!=0]\n",
    "    Non_SAR = data[data[SAR]==0]\n",
    "    print('total number of TO_SAR:', TO_SAR.shape[0])\n",
    "    print('*'*32)\n",
    "    train_TO_SAR = TO_SAR[:int(TO_SAR.shape[0] * p)]\n",
    "    train_Non_SAR = Non_SAR[:int(Non_SAR.shape[0] * p)]\n",
    "    \n",
    "    train = train_Non_SAR.append(train_TO_SAR)\n",
    "    train = train.sort_values(by=['run_date'])\n",
    "   \n",
    "    print('time interval in train set:{}~{}'.format(train.iloc[0]['run_date'], train.iloc[train.shape[0]-1]['run_date']))\n",
    "    print('TO_SAR in train set:', train_TO_SAR.shape[0])\n",
    "    print('TO_SAR/Total in train set', train_TO_SAR.shape[0] / (train_Non_SAR.shape[0] + train_TO_SAR.shape[0]))\n",
    "    print('split dataset to train, test...')\n",
    "    print('*'*32)\n",
    "    test_TO_SAR = TO_SAR[int(TO_SAR.shape[0] * p):]\n",
    "    test_Non_SAR = Non_SAR[int(Non_SAR.shape[0]*(p)):]\n",
    "    test = test_Non_SAR.append(test_TO_SAR)\n",
    "    diff = set(train) - set(test)\n",
    "    if len(diff)!=0:\n",
    "        raise ValueError\n",
    "    test = test.sort_values(by=['run_date'])\n",
    "    print('time interval in test set:{}~{}'.format(test.iloc[0]['run_date'], test.iloc[test.shape[0]-1]['run_date']))\n",
    "    print('TO_SAR in test set:', test_TO_SAR.shape[0])\n",
    "    print('TO_SAR/Total in test set', test_TO_SAR.shape[0] / (test_Non_SAR.shape[0]+ test_TO_SAR.shape[0]))\n",
    "    print('*'*32)\n",
    "    \n",
    "    train = train.drop(['run_date'], axis = 1)\n",
    "    test = test.drop(['run_date'], axis = 1)\n",
    "    print('train data shape: ', train.shape)\n",
    "    print('test data shape: ', test.shape)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# file\n",
    "def ReadFile(df, col):\n",
    "    df = df.drop(['alert_id','primary_entity_level_code','primary_entity_number', 'scenario_name','Cust_No','Acct_No'], axis = 1)\n",
    "    ratio = 0.6\n",
    "    test_set_type = 0\n",
    "    train, test = SplitDataset(df, ratio, test_set_type, col)\n",
    "    train = train.reset_index(drop = True)\n",
    "    test = test.reset_index(drop = True)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def getData(df, ratio, oversample, test_set_type, col):\n",
    "    \n",
    "    train, test= SplitDataset(df, ratio, test_set_type, col)\n",
    "         \n",
    "    print('TO_SAR=1:To_SAR=0 = {}:{}'.format(list(train['TO_SAR'].values).count(1), list(train['TO_SAR'].values).count(0) ))\n",
    "    print('*'*32)\n",
    "    \n",
    "    if oversample == 0:\n",
    "        print('Not doing oversampling...')\n",
    "    elif oversample == 1:\n",
    "        print('Oversampling by duplicate TO SAR=1 data...')\n",
    "        resample_ratio = train[train['TO_SAR']==0].shape[0] / train[train['TO_SAR']==1].shape[0]\n",
    "        #print('resample ratio:', resample_ratio)\n",
    "        train = train.append([train[train['TO_SAR']==1]]*(int(resample_ratio)-1))\n",
    "        print('TO SAR=1:To SAR=0 ={}:{} in train set'.format(list(train['TO_SAR'].values).count(1), list(train['TO_SAR'].values).count(0) ))\n",
    "   \n",
    "\n",
    "    print('fradulent in train/test:'.format(list(train['TO_SAR'].values).count(1),\n",
    "                                            list(test['TO_SAR'].values).count(1)))\n",
    "   \n",
    "    train_data = train.drop(['TO_SAR'], axis = 1).to_numpy()\n",
    "    train_label = train['TO_SAR'].to_numpy()\n",
    "    test_data = test.drop(['TO_SAR'], axis = 1).to_numpy()\n",
    "    test_label = test['TO_SAR'].to_numpy()\n",
    "   \n",
    "    return train_data, train_label, test_data, test_label, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResult(y_true, y_pred):\n",
    "    results = pd.DataFrame(columns = ['SAR', 'Non SAR','newSAM=1_SAR=1(TP)','newSAM=1_SAR=0(FP)',\\\n",
    "                                      'newSAM=0_SAR=1(FN)', 'newSAM=0_SAR=0(TN)', 'recall','filter rate'])\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_true[i]==y_pred[i]==1:\n",
    "            TP += 1\n",
    "        if y_pred[i]==1 and y_true[i]==0: \n",
    "            FP += 1\n",
    "        if y_true[i]==y_pred[i]==0:\n",
    "            TN += 1\n",
    "        if y_pred[i]==0 and y_true[i]==1:\n",
    "            FN += 1\n",
    "\n",
    "    HRS = (TN+FN)/(TN+FN+TP+FP)\n",
    "   \n",
    "    results = results.append({'SAR':list(y_true).count(1), \\\n",
    "                              'Non SAR':list(y_true).count(0),\\\n",
    "                              'newSAM=1_SAR=1(TP)':TP, 'newSAM=1_SAR=0(FP)':FP, 'newSAM=0_SAR=1(FN)':FN,\\\n",
    "                              'newSAM=0_SAR=0(TN)':TN, 'recall': str(round(TP/(TP+FN), 4)),\\\n",
    "                              'filter rate':str((round(HRS, 4)))}, ignore_index = True)\n",
    "    display(results)\n",
    "\n",
    "\n",
    "def train(threshold_test, train_data, train_label, test_data, test_label):\n",
    "    print('Training session starts...')\n",
    "    model = XGBClassifier(use_label_encoder = False, eval_metric = 'logloss', n_jobs = -1).fit(train_data, train_label)\n",
    "    if threshold_test:\n",
    "        recalls = []\n",
    "        HRSs = []\n",
    "        for th in np.arange(1e-10, 1e-9, 1e-10):\n",
    "            print('threshold = ', th)\n",
    "            predicted_proba = model.predict_proba(test_data)\n",
    "            y_pred = (predicted_proba[:,1]>=th)\n",
    "            getResult(test_label,y_pred)\n",
    "        \n",
    "    else:\n",
    "        predicted_proba = model.predict_proba(test_data)\n",
    "        y_pred = (predicted_proba[:,1]>=0.5)\n",
    "        getResult(test_label, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawFeatureDistribution(data, feature, day, label):\n",
    "    data[feature].value_counts() # 存款金額\n",
    "    #print(data[feature].value_counts())\n",
    "    y, x = data[feature].value_counts().values, data[feature].value_counts().index\n",
    "    plt.figure()\n",
    "    plt.title(\"distribution of {} of SAR={} in past {} days\".format(feature, label, day))\n",
    "    plt.xlabel(\"{} in past {} days\".format(feature, day))\n",
    "    plt.ylabel(\"number of data\")\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter settings\n",
    "RANK = 10\n",
    "SLICE = 10\n",
    "beta = 2 # give recall weight=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 閥值x>=且閥值y>=\n",
    "def Search2D(xRange, yRange, x, y, defaultX, defaultY, SAR, Non_SAR, train_set, LOGIC, SAR_type):\n",
    "    # x, y: feature\n",
    "    numberOfSAR = [[0] * SLICE for i in range(SLICE)] #sar個數\n",
    "    recallRate = [[0] * SLICE for i in range(SLICE)]\n",
    "    filterRate = [[0] * SLICE for i in range(SLICE)]\n",
    "    harmonicRecallFilter = [[0] * SLICE for i in range(SLICE)] # apply f1-score simulated formula\n",
    "    for i in range(len(xRange)):\n",
    "        for j in range(len(yRange)):\n",
    "            thre1 = xRange[i]\n",
    "            thre2 = yRange[j] \n",
    "            if thre1 < defaultX or thre2 < defaultY:\n",
    "                continue\n",
    "            idx1 = set(train_set[train_set[y] >= thre2].index)\n",
    "            idx2 = set(train_set[train_set[x] >= thre1].index)\n",
    "\n",
    "            idx = list(idx1 & idx2) # 閥值x>= \"且\" 閥值y>=\n",
    "            new_SAM_1 = train_set.iloc[idx]\n",
    "            new_SAM_0 = train_set.drop(index = new_SAM_1.index)\n",
    "    \n",
    "            if len(set(new_SAM_0.index) & set(new_SAM_1.index)) != 0:\n",
    "                raise ValueError(\"new SAM 0 and new SAM 1 overlapped!\")\n",
    "\n",
    "            TP = new_SAM_1[new_SAM_1[SAR_type] != 0].shape[0]\n",
    "            TN = new_SAM_0[new_SAM_0[SAR_type] == 0].shape[0]\n",
    "            FP = new_SAM_1[new_SAM_1[SAR_type] == 0].shape[0]\n",
    "            FN = new_SAM_0[new_SAM_0[SAR_type] != 0].shape[0]\n",
    "        \n",
    "            numberOfSAR[i][j] = TP\n",
    "            recallRate[i][j] = TP / (TP + FN)\n",
    "            filterRate[i][j] = (TN + FN) / (TP + FP + TN + FN)\n",
    "            harmonicRecallFilter[i][j] = ((1 + beta * beta) * recallRate[i][j] * filterRate[i][j])\\\n",
    "            / (recallRate[i][j] + beta * beta * filterRate[i][j])\n",
    "            #print(thre1, thre2, 'TP:',TP,  'FP:',FP,  'FN:',FN,  'TN:',TN,  'recall:',recallRate[i][j] , 'filter:', filterRate[i][j], harmonicRecallFilter[i][j])\n",
    "            \n",
    "    if LOGIC == \"AND\":\n",
    "        mat = recallRate\n",
    "    elif LOGIC == \"OR\":\n",
    "        mat = harmonicRecallFilter\n",
    "    \n",
    "    max_pos = [] \n",
    "    maxVal = np.amax(mat) # find the biggest values\n",
    "    #print('max val', maxVal)\n",
    "    countMax = 0\n",
    "    countZero = 0\n",
    "    for i in range(len(mat)):\n",
    "        for j in range(len(mat[0])):\n",
    "            if mat[i][j] == maxVal:\n",
    "                countMax += 1\n",
    "            if mat[i][j] == 0:\n",
    "                countZero += 1\n",
    "                \n",
    "    print('max val: ', maxVal, 'number of max val: ', countMax)\n",
    "    print(f'Rank: {RANK}, countMax: {countMax}, {SLICE}- countZero: {SLICE**2 -countZero}')\n",
    "    for i in range(min(max(RANK, countMax), SLICE**2 - countZero)):\n",
    "        idx = np.array(mat).argmax() # find the index where the biggest values is\n",
    "        max_pos.append(idx)\n",
    "        print('{}th, recall: {}, val1: {}, val2: {}'.format(idx, recallRate[int(idx / SLICE)][idx % SLICE], \\\n",
    "                                                            xRange[int(idx / SLICE)],yRange[idx % SLICE]))\n",
    "        mat[int(idx / SLICE)][idx % SLICE]=-1\n",
    "        \n",
    "    print('*'*32)\n",
    "    candidate_rule = []\n",
    "    for i in range(min(RANK, SLICE**2 - countZero)):\n",
    "        idx = max_pos[-(i + 1)]\n",
    "        val0 = recallRate[int(idx / SLICE)][idx % SLICE]\n",
    "        val1 = xRange[int(idx / SLICE)]\n",
    "        val2 = yRange[idx % SLICE]\n",
    "        candidate_rule.append([val1, val2])\n",
    "   \n",
    "    \"\"\"results = pd.DataFrame(columns = [x,y,'SAR', 'Non SAR','newSAM=1_SAR=1(TP)','newSAM=1_SAR=0(FP)',\\\n",
    "                                  'newSAM=0_SAR=1(FN)', 'newSAM=0_SAR=0(TN)', 'recall','filter rate'])\n",
    "    \n",
    "    for thre1, thre2 in candidate_rule:\n",
    "        idx = list(set(train_set[train_set[x] >= thre1].index) & set(train_set[train_set[y] >= thre2].index))\n",
    "        new_SAM_1_rule1 = train_set.iloc[idx]\n",
    "        new_SAM_1 = new_SAM_1_rule1\n",
    "        new_SAM_1 = new_SAM_1.drop_duplicates()\n",
    "        new_SAM_0 = train_set.drop(index = new_SAM_1.index)\n",
    "\n",
    "        if (new_SAM_0.shape[0] + new_SAM_1.shape[0]) != train_set.shape[0]:\n",
    "            raise ValueError(\"new SAM 0 and new SAM 1 overlapped!\")\n",
    "            \n",
    "        TP = new_SAM_1[new_SAM_1[SAR_type] != 0].shape[0]\n",
    "        TN = new_SAM_0[new_SAM_0[SAR_type] == 0].shape[0]\n",
    "        FP = new_SAM_1[new_SAM_1[SAR_type] == 0].shape[0]\n",
    "        FN = new_SAM_0[new_SAM_0[SAR_type] != 0].shape[0]\n",
    "        total = train_set.shape[0]\n",
    "        results = results.append({x:thre1, y:thre2,'SAR':train_set[train_set[SAR_type] != 0].shape[0], \\\n",
    "                                  'Non SAR':train_set[train_set[SAR_type] == 0].shape[0],\\\n",
    "                                  'newSAM=1_SAR=1(TP)':TP, 'newSAM=1_SAR=0(FP)':FP, 'newSAM=0_SAR=1(FN)':FN,\\\n",
    "                                  'newSAM=0_SAR=0(TN)':TN, 'recall': str(round(TP/(TP + FN), 4)),\\\n",
    "                                  'filter rate':str((round((TN + FN) / total, 4)))}, ignore_index = True)\n",
    "    display(results)\"\"\"\n",
    "    \n",
    "    return candidate_rule\n",
    "\n",
    "\n",
    "def Search1D(xRange, x, defaultX, SAR, Non_SAR, train_set, LOGIC, SAR_type):\n",
    "    numberOfSAR = [0] * (2 * SLICE - 1) #sar個數\n",
    "    recallRate = [0] * (2 * SLICE - 1) \n",
    "    filterRate= [0] *(2 * SLICE - 1)  \n",
    "    harmonicRecallFilter = [0] * (2 * SLICE - 1)  # apply f1-score simulated formula\n",
    "    \n",
    "    for i in range(len(xRange)):\n",
    "        thre1 = xRange[i]\n",
    "        \n",
    "        if thre1 < defaultX:\n",
    "            continue\n",
    "            \n",
    "        idx1 = set(train_set[train_set[x] >= thre1].index)\n",
    "        idx = list(idx1) # 閥值x>= \n",
    "\n",
    "        new_SAM_1_rule1 = train_set.iloc[idx]\n",
    "        new_SAM_1 = new_SAM_1_rule1\n",
    "        new_SAM_1 = new_SAM_1.drop_duplicates()\n",
    "        new_SAM_0 = train_set.drop(index = new_SAM_1.index)\n",
    "\n",
    "        if (new_SAM_0.shape[0] + new_SAM_1.shape[0]) != train_set.shape[0]:\n",
    "            raise ValueError(\"new SAM 0 and new SAM 1 overlapped!\")\n",
    "\n",
    "        TP = new_SAM_1[new_SAM_1[SAR_type] != 0].shape[0]\n",
    "        TN = new_SAM_0[new_SAM_0[SAR_type] == 0].shape[0]\n",
    "        FP = new_SAM_1[new_SAM_1[SAR_type] == 0].shape[0]\n",
    "        FN = new_SAM_0[new_SAM_0[SAR_type] != 0].shape[0]\n",
    "    \n",
    "        numberOfSAR[i] = TP\n",
    "        recallRate[i] = TP / (TP + FN)\n",
    "        filterRate[i] = (TN + FN)/(TP + FP + TN + FN)\n",
    "        harmonicRecallFilter[i] = float((1 + beta * beta) * recallRate[i] * filterRate[i]\\\n",
    "                                        / (recallRate[i] + beta * beta * filterRate[i]))\n",
    "        \n",
    "        print(recallRate[i])\n",
    "        \n",
    "    if LOGIC == \"AND\":\n",
    "        mat = recallRate\n",
    "    elif LOGIC == \"OR\":\n",
    "        mat = harmonicRecallFilter\n",
    "\n",
    "        \n",
    "    print('recall', mat)\n",
    "    max_pos = [] \n",
    "    maxVal = max(mat)\n",
    "    countMax = mat.count(maxVal)\n",
    "    countZero = mat.count(0)\n",
    "    \n",
    "    print('max val: ', maxVal, 'number of max val: ', countMax)\n",
    "    print('number of zero:', countZero)\n",
    "    for i in range(min(max(RANK, countMax), 19-countZero)):\n",
    "        idx = np.array(mat).argmax()\n",
    "        max_pos.append(idx)\n",
    "        print('{}th, recall: {} , val: {}'.format(idx, recallRate[idx % (2 * SLICE - 1)], xRange[idx % (2 * SLICE - 1)]))\n",
    "        mat[idx % (2 * SLICE - 1)] = -1\n",
    "        \n",
    "    candidate_rule = []\n",
    "    \n",
    "    for i in range(min(RANK, 19 - countZero)):\n",
    "        idx = max_pos[-(i + 1)]\n",
    "        x = xRange[idx % (2 * SLICE - 1)]\n",
    "        candidate_rule.append(x)\n",
    "        \n",
    "    \n",
    "    results = pd.DataFrame(columns = [x, 'SAR', 'Non SAR','newSAM=1_SAR=1(TP)','newSAM=1_SAR=0(FP)',\\\n",
    "                                  'newSAM=0_SAR=1(FN)', 'newSAM=0_SAR=0(TN)', 'recall','filter rate'])\n",
    "    \n",
    "    \"\"\"for thre1 in candidate_rule:\n",
    "        print(thre1)\n",
    "        display(train_set[train_set[x] >= float(thre1)])\n",
    "        idx = train_set[train_set[x] >= thre1].index\n",
    "        new_SAM_1 = train_set.iloc[idx]\n",
    "        new_SAM_0 = train_set.drop(index = new_SAM_1.index)\n",
    "\n",
    "        if (new_SAM_0.shape[0] + new_SAM_1.shape[0]) != train_set.shape[0]:\n",
    "            raise ValueError(\"new SAM 0 and new SAM 1 overlapped!\")\n",
    "            \n",
    "        TP = new_SAM_1[new_SAM_1[SAR_type] != 0].shape[0]\n",
    "        TN = new_SAM_0[new_SAM_0[SAR_type] == 0].shape[0]\n",
    "        FP = new_SAM_1[new_SAM_1[SAR_type] == 0].shape[0]\n",
    "        FN = new_SAM_0[new_SAM_0[SAR_type] != 0].shape[0]\n",
    "        total = train_set.shape[0]\n",
    "        results = results.append({x:thre1, 'SAR':train_set[train_set[SAR_type] != 0].shape[0], \\\n",
    "                                  'Non SAR':train_set[train_set[SAR_type] == 0].shape[0],\\\n",
    "                                  'newSAM=1_SAR=1(TP)':TP, 'newSAM=1_SAR=0(FP)':FP, 'newSAM=0_SAR=1(FN)':FN,\\\n",
    "                                  'newSAM=0_SAR=0(TN)':TN, 'recall': str(round(TP/(TP + FN), 4)),\\\n",
    "                                  'filter rate':str((round((TN + FN) / total, 4)))}, ignore_index = True)\n",
    "    \n",
    "    display(results)\"\"\"\n",
    "\n",
    "    return candidate_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(data, feature):\n",
    "    total_cnt = len(data)\n",
    "    if total_cnt == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        target_cnt = np.array([len(data[data[feature] == 0]), len(data[data[feature] != 0])])\n",
    "        pk = target_cnt / total_cnt\n",
    "        vec = entr(pk)\n",
    "        S = np.sum(vec, axis= 0)\n",
    "        S /= np.log(2) # 換底公式\n",
    "        return S\n",
    "\n",
    "def SplitMedian(data, feature):\n",
    "    split_number = np.unique(train_set[feature])\n",
    "    split_number = split_number[~np.isnan(split_number)]\n",
    "    split_number.sort(axis= 0)\n",
    "    median = np.empty(len(split_number) - 1)\n",
    "    for i in range(1, len(split_number)):\n",
    "        median[i-1] = (split_number[i] + split_number[i-1]) / 2\n",
    "    print(f'len of median: {len(median)}')\n",
    "    \n",
    "    return median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter settings\n",
    "RANK = 10\n",
    "SLICE = 10\n",
    "beta = 2 # give recall weight=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search2Rule(xRange, yRange, x, y, defaultX, defaultY, SAR, Non_SAR, train_set, LOGIC, SAR_type):\n",
    "    numberOfSAR = [[0] * SLICE for i in range(SLICE)] # SAR個數\n",
    "    recallRate = [[0] * SLICE for i in range(SLICE)]\n",
    "    filterRate = [[0] * SLICE for i in range(SLICE)]\n",
    "    harmonicRecallFilter = [[0] * SLICE for i in range(SLICE)]\n",
    "    information_gain = [[0] * SLICE for i in range(SLICE)]\n",
    "    total_cnt = train_set.shape[0]\n",
    "    father_entropy = Entropy(train_set, 'TO_SAR')\n",
    "    \n",
    "#     results = pd.DataFrame(columns = [x, y, 'information gain','SAR', 'Non SAR','newSAM=1_SAR=1(TP)','newSAM=1_SAR=0(FP)',\\\n",
    "#                                   'newSAM=0_SAR=1(FN)', 'newSAM=0_SAR=0(TN)', 'recall','filter rate'])\n",
    "    \n",
    "    for i in range(len(xRange)):\n",
    "        for j in range(len(yRange)):\n",
    "            thre1 = xRange[i]\n",
    "            thre2 = yRange[j]\n",
    "            if thre1 < defaultX or thre2 < defaultY:\n",
    "                continue\n",
    "            idx1 = set(train_set[train_set[x] >= thre1].index)\n",
    "            idx2 = set(train_set[train_set[y] >= thre2].index)\n",
    "        \n",
    "            idx = list(idx1 & idx2) # 閥值x>= \"且\" 閥值y>=\n",
    "            new_SAM_1 = train_set.iloc[idx]\n",
    "            new_SAM_0 = train_set.drop(index= new_SAM_1.index)\n",
    "        \n",
    "            if len(set(new_SAM_0.index) & set(new_SAM_1.index)) != 0:\n",
    "                raise ValueError('new SAM 0 and new SAM 1 overlapped!')\n",
    "            \n",
    "            if len(new_SAM_0) == 0 or len(new_SAM_1) == 0:\n",
    "                continue\n",
    "        \n",
    "            TP = new_SAM_1[new_SAM_1[SAR_type] != 0].shape[0]\n",
    "            TN = new_SAM_0[new_SAM_0[SAR_type] == 0].shape[0]\n",
    "            FP = new_SAM_1[new_SAM_1[SAR_type] == 0].shape[0]\n",
    "            FN = new_SAM_0[new_SAM_0[SAR_type] != 0].shape[0]\n",
    "        \n",
    "            numberOfSAR[i][j] = TP\n",
    "            recallRate[i][j] = TP / (TP + FN)\n",
    "            filterRate[i][j] = (TN + FN) / (TP + FP + TN + FN)\n",
    "            harmonicRecallFilter[i][j] = ((1 + beta * beta) * recallRate[i][j] * filterRate[i][j]) \\\n",
    "                        / (recallRate[i][j] + beta * beta * filterRate[i][j])\n",
    "            #print(f'new_SAM_0: {len(new_SAM_0)}, new_SAM_1: {len(new_SAM_1)}, val1: {thre1}, val2: {thre2}')\n",
    "            condition_entropy_0 = Entropy(new_SAM_0, 'TO_SAR') * (len(new_SAM_0) / total_cnt)\n",
    "            condition_entropy_1 = Entropy(new_SAM_1, 'TO_SAR') * (len(new_SAM_1) / total_cnt)\n",
    "            information_gain[i][j] = father_entropy - (condition_entropy_0 + condition_entropy_1)\n",
    "            \n",
    "            \n",
    "#             results = results.append({x:thre1, y: thre2, 'information gain': information_gain[i][j], 'SAR':SAR.shape[0], \\\n",
    "#                             'Non SAR':Non_SAR.shape[0],\\\n",
    "#                             'newSAM=1_SAR=1(TP)':TP, 'newSAM=1_SAR=0(FP)':FP, 'newSAM=0_SAR=1(FN)':FN,\\\n",
    "#                             'newSAM=0_SAR=0(TN)':TN, 'recall': recallRate[i][j],\\\n",
    "#                             'filter rate':filterRate[i][j]}, ignore_index = True)\n",
    "               \n",
    "    if LOGIC == 'AND':\n",
    "        mat = recallRate\n",
    "        #mat = information_gain\n",
    "    elif LOGIC == 'OR':\n",
    "        mat = harmonicRecallFilter\n",
    "        #mat = information_gain\n",
    "        \n",
    "    max_pos = []\n",
    "    maxVal = np.amax(mat)\n",
    "    countMax = 0\n",
    "    countZero = 0\n",
    "    \n",
    "    for i in range(len(mat)):\n",
    "        for j in range(len(mat[0])):\n",
    "            if mat[i][j] == maxVal:\n",
    "                countMax += 1\n",
    "            if mat[i][j] == 0:\n",
    "                countZero += 1\n",
    "    print('max val: ', maxVal, 'number of max val: ', countMax)\n",
    "    for i in range(min(max(RANK, countMax), SLICE - countZero)):\n",
    "        idx = np.array(mat).argmax()\n",
    "        max_pos.append(idx)\n",
    "        print('{}th, information gain: {}, recall: {}, val1: {}, val2: {}'.format(idx \\\n",
    "                , information_gain[int(idx / SLICE)][idx % SLICE], recallRate[int(idx / SLICE)][idx % SLICE] \\\n",
    "                , xRange[int(idx / SLICE)], yRange[int(idx % SLICE)]))\n",
    "        mat[int(idx / SLICE)][idx % SLICE] = -1\n",
    "    \n",
    "    print('*' * 32)\n",
    "    candidate_rule = []\n",
    "    for i in range(min(RANK, SLICE - countZero)):\n",
    "        idx = max_pos[-(i + 1)]\n",
    "        val_IG = information_gain[int(idx / SLICE)][idx % SLICE]\n",
    "        val_recall = recallRate[int(idx / SLICE)][idx % SLICE]\n",
    "        val1 = xRange[int(idx / SLICE)]\n",
    "        val2 = yRange[idx % SLICE]\n",
    "        candidate_rule.append([val_IG, val_recall, val1, val2])\n",
    "    \n",
    "    return candidate_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newton method funcion for k dimension\n",
    "def informationGain(point, feature_name, data):\n",
    "    N = len(feature_name)\n",
    "    idx = []\n",
    "    for i, name in enumerate(feature_name):\n",
    "        idx.append(set(data[data[name] > float(point[i])].index))\n",
    "    \n",
    "    intersect_idx = idx[0]\n",
    "\n",
    "    for i in range(1, N):\n",
    "        intersect_idx = intersect_idx & idx[i]\n",
    "    intersect_idx = list(intersect_idx)\n",
    "\n",
    "    new_sam_1 = data.iloc[intersect_idx]\n",
    "    new_sam_0 = data.drop(index= new_sam_1.index)\n",
    "    #print(len(new_sam_1), len(new_sam_0))\n",
    "    if len(set(new_sam_1.index) & set(new_sam_0.index)) != 0:\n",
    "        raise ValueError('new sam 0 and new sam 1 are overlapped')\n",
    "        \n",
    "    return -(father_entropy - conditionalEntropy(new_sam_0, new_sam_1))\n",
    "\n",
    "def conditionalEntropy(new_sam_0, new_sam_1):\n",
    "    total = len(new_sam_1) + len(new_sam_0)\n",
    "    return Entropy(new_sam_0, SAR_type) * (len(new_sam_0) / total) \\\n",
    "            + Entropy(new_sam_1, SAR_type) * (len(new_sam_1) / total)\n",
    "\n",
    "\n",
    "def gradientIG(point, feature_name, data):\n",
    "    K = len(feature_name)\n",
    "    D = 1\n",
    "    \n",
    "    while True:\n",
    "        _, auxiliary_idx  = kdtree.query(point.flatten(), k= D)\n",
    "        if D == 1:\n",
    "            auxiliary = data.loc[auxiliary_idx, feature_name].to_numpy(dtype= 'float64').reshape(K, -1)\n",
    "        else:\n",
    "            auxiliary_idx = auxiliary_idx[-1]\n",
    "            auxiliary = data.loc[auxiliary_idx, feature_name].to_numpy(dtype= 'float64').reshape(K, -1)\n",
    "        \n",
    "        if np.all(auxiliary != point):\n",
    "            break\n",
    "        D += 10\n",
    "    \n",
    "    delta = auxiliary - point\n",
    "    tmp = []\n",
    "    for i in range(K):\n",
    "        tmp.append(point.copy())\n",
    "\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            if i == j:\n",
    "                tmp[i][j] = auxiliary[i]\n",
    "    \n",
    "    z = informationGain(point, feature_name, data)\n",
    "    z_delta = []\n",
    "\n",
    "    for i in range(K):\n",
    "        z_delta.append(informationGain(tmp[i], feature_name, data))\n",
    "    \n",
    "    grad = (np.asarray(z_delta).reshape((K, -1)) - z) / delta\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(point, feature_name, data):\n",
    "    N = len(feature_name)\n",
    "    idx = []\n",
    "    for i, name in enumerate(feature_name):\n",
    "        idx.append(set(data[data[name] >= float(point[i])].index))\n",
    "    \n",
    "    intersect_idx = idx[0]\n",
    "\n",
    "    for i in range(1, N):\n",
    "        intersect_idx = intersect_idx & idx[i]\n",
    "    intersect_idx = list(intersect_idx)\n",
    "\n",
    "    new_sam_1 = data.iloc[intersect_idx]\n",
    "    new_sam_0 = data.drop(index= new_sam_1.index)\n",
    "\n",
    "    if len(set(new_sam_1.index) & set(new_sam_0.index)) != 0:\n",
    "        raise ValueError('new sam 1 and new sam 0 are overlapped')\n",
    "    \n",
    "    TP = new_sam_1[new_sam_1[SAR_type] != 0].shape[0]\n",
    "    TN = new_sam_0[new_sam_0[SAR_type] == 0].shape[0]\n",
    "    FP = new_sam_1[new_sam_1[SAR_type] == 0].shape[0]\n",
    "    FN = new_sam_0[new_sam_0[SAR_type] != 0].shape[0]\n",
    "    \n",
    "    return -(TP / (TP + FN))\n",
    "\n",
    "def RecallFilterRate(point, feature_name, data):\n",
    "    N = len(feature_name)\n",
    "    idx = []\n",
    "    for i, name in enumerate(feature_name):\n",
    "        idx.append(set(data[data[name] >= float(point[i])].index))\n",
    "    \n",
    "    intersect_idx = idx[0]\n",
    "\n",
    "    for i in range(1, N):\n",
    "        intersect_idx = intersect_idx & idx[i]\n",
    "    intersect_idx = list(intersect_idx)\n",
    "\n",
    "    new_sam_1 = data.iloc[intersect_idx]\n",
    "    new_sam_0 = data.drop(index= new_sam_1.index)\n",
    "\n",
    "    if len(set(new_sam_1.index) & set(new_sam_0.index)) != 0:\n",
    "        raise ValueError('new sam 1 and new sam 0 are overlapped')\n",
    "    \n",
    "    TP = new_sam_1[new_sam_1[SAR_type] != 0].shape[0]\n",
    "    TN = new_sam_0[new_sam_0[SAR_type] == 0].shape[0]\n",
    "    FP = new_sam_1[new_sam_1[SAR_type] == 0].shape[0]\n",
    "    FN = new_sam_0[new_sam_0[SAR_type] != 0].shape[0]\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    filter_rate = (TN + FN) / (TP + TN + FP + FN)\n",
    "\n",
    "    return recall, filter_rate\n",
    "\n",
    "\n",
    "def gradientPenalty(point, upperbound=np.zeros((2,1))):\n",
    "    if np.any(point < 0): # any 是只要有一個是 True 就 output True，但 all 是要全部都為 True 才 output True\n",
    "        raise ValueError('point are negative!')\n",
    "\n",
    "    if np.linalg.norm(upperbound) == 0:\n",
    "        return 1 / point\n",
    "    else:\n",
    "        return 1 / (upperbound - point)\n",
    "\n",
    "\n",
    "def gradientRecall(point, feature_name, data):\n",
    "    N = len(feature_name)\n",
    "    K = 1\n",
    "    \n",
    "    while True:\n",
    "        _, auxiliary_idx  = kdtree.query(point.flatten(), k= K)\n",
    "        if K == 1:\n",
    "            auxiliary = data.loc[auxiliary_idx, feature_name].to_numpy().reshape(N, -1)\n",
    "        else:\n",
    "            auxiliary_idx = auxiliary_idx[-1]\n",
    "            auxiliary = data.loc[auxiliary_idx, feature_name].to_numpy().reshape(K, -1)\n",
    "        \n",
    "        if auxiliary.all() != point.all():\n",
    "            break\n",
    "        K += 1    \n",
    "\n",
    "    delta = auxiliary - point\n",
    "    tmp = []\n",
    "    for i in range(N):\n",
    "        tmp.append(point)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                tmp[i][j] = auxiliary[i]\n",
    "    \n",
    "    z = Recall(point, feature_name, data)\n",
    "    z_delta = []\n",
    "\n",
    "    for i in range(N):\n",
    "        z_delta.append(Recall(tmp[i], feature_name, data))\n",
    "    grad = (z_delta - z) / delta\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-(sigmoid_b + sigmoid_w * x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidDiff(x):\n",
    "    tmp = sigmoid(x)\n",
    "    return tmp * (1 - tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    x = list(results.loc[results['iteration number'] % 50 == 0].index)\n",
    "    x.append(len(results) - 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, results.loc[x, 'information gain'], 'o--')\n",
    "    plt.plot(x, results.loc[x, 'recall'], 'o--')\n",
    "    plt.plot(x, results.loc[x, 'filter rate'], 'o--')\n",
    "    plt.xlabel('iteration number')\n",
    "\n",
    "    plt.legend(['information gain', 'recall', 'filter rate'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seperator(results):\n",
    "    x = list(results.loc[results['iteration number'] % 50 == 0].index)\n",
    "    x.append(len(results) - 1)\n",
    "\n",
    "    plot_name = ['information gain', 'recall', 'filter rate', 'loss']\n",
    "    color = ['r', 'b', 'g', 'c']\n",
    "    plt.figure()\n",
    "    for i, name in enumerate(plot_name):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.plot(x, results.loc[x, name], 'o--', color= color[i])\n",
    "        plt.title(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c70b165adba5618d06f74f9d07fe6485cbd83160061bc4485c0745b1939ab6c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
