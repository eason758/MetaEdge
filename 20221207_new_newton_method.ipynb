{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import entr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree, KDTree\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alert_id', 'run_date', 'primary_entity_level_code',\n",
       "       'primary_entity_number', 'scenario_name', 'TO_SAR', 'Acct_No',\n",
       "       'Cust_No', 'Cust_Key', 'Cust_Segmentation', 'Cust_Typ_Cd', 'Cust_Id_Id',\n",
       "       'Cust_Id_Typ_Desc', 'Cust_Id_State_Cd', 'Cust_Dt_Of_Birth',\n",
       "       'Cust_Sts_Desc', 'Street_Postal_Cd', 'Street_Cntry_Cd',\n",
       "       'Mailing_Postal_Cd', 'Mailing_Cntry_Cd', 'Residence_Cntry_Cd',\n",
       "       'Citizenship_Cntry_Cd', 'Org_Cntry_Of_Buz_Cd', 'Empl_No', 'Ocup_Code',\n",
       "       'job_title_code', 'Lst_Cont_Dt', 'Politically_Exposed_Pers_Ind',\n",
       "       'Non_Prft_Org_Ind', 'Cust_Since_Dt', 'Lst_Susp_Actv_Rpt_Dt',\n",
       "       'Extnl_Cust_Ind', 'Negative_News_Ind', 'Prim_Br_No', 'Obu_Ind',\n",
       "       'Riskclass_Cd', 'risk_score', 'last_risk_assessment_date',\n",
       "       'change_current_ind', 'change_begin_date', 'change_end_date',\n",
       "       'Credit_Amt', 'number_of_Credit', 'Debit_Amt', 'number_of_Debit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'C:/Users/NCTUUser2/Desktop/mark/'\n",
    "FILE = 'TWN_A11_01_day15_with_aggregated_txn_scenario_txn_type_key_營業日.csv'\n",
    "df = pd.read_csv(PATH + FILE)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TO_SAR'] = df['TO_SAR'].replace(np.nan, 0)\n",
    "df['TO_SAR'] = df['TO_SAR'].replace('F', 1)\n",
    "df['TO_SAR'] = df['TO_SAR'].replace('NF', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify on time series\n",
      "shape of data: (86770, 39)\n",
      "total number of TO_SAR: 446\n",
      "********************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NCTUUser2\\AppData\\Local\\Temp\\4\\ipykernel_11576\\2736333543.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train_Non_SAR.append(train_TO_SAR)\n",
      "C:\\Users\\NCTUUser2\\AppData\\Local\\Temp\\4\\ipykernel_11576\\2736333543.py:27: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test = test_Non_SAR.append(test_TO_SAR)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time interval in train set:2019-03-04~2020-04-29\n",
      "TO_SAR in train set: 267\n",
      "TO_SAR/Total in train set 0.005128599143312653\n",
      "split dataset to train, test...\n",
      "********************************\n",
      "time interval in test set:2020-02-19~2020-12-31\n",
      "TO_SAR in test set: 179\n",
      "TO_SAR/Total in test set 0.0051571638479933155\n",
      "********************************\n",
      "train data shape:  (52061, 38)\n",
      "test data shape:  (34709, 38)\n"
     ]
    }
   ],
   "source": [
    "SAR_type = 'TO_SAR'\n",
    "train_set, test_set = ReadFile(df, SAR_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set:(52060, 38)\n",
      "test_set:(34709, 38)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_set.dropna(subset= ['Credit_Amt', 'number_of_Credit', 'Debit_Amt', 'number_of_Debit']).reset_index(drop= True)\n",
    "test_set = test_set.dropna(subset= ['Credit_Amt', 'number_of_Credit', 'Debit_Amt', 'number_of_Debit']).reset_index(drop= True)\n",
    "print(f'train_set:{train_set.shape}\\n' + f'test_set:{test_set.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR = train_set[train_set.TO_SAR!= 0]\n",
    "Non_SAR = train_set[train_set.TO_SAR== 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(data, feature):\n",
    "    total_cnt = len(data)\n",
    "    if total_cnt == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        target_cnt = np.array([len(data[data[feature] == 0]), len(data[data[feature] != 0])])\n",
    "        pk = target_cnt / total_cnt\n",
    "        vec = entr(pk)\n",
    "        S = np.sum(vec, axis= 0)\n",
    "        S /= np.log(2) # 換底公式\n",
    "        return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientIG(point, x_name, y_name, data):\n",
    "    K = 2\n",
    "    while True:\n",
    "        _, auxiliary_idx = credit_kdtree.query(point.flatten(), k= K)\n",
    "        auxiliary_idx = auxiliary_idx[-1]\n",
    "        auxiliary = data.loc[auxiliary_idx, [x_name, y_name]].to_numpy().reshape(2,1)\n",
    "\n",
    "        if auxiliary.all() != point.all():\n",
    "            break\n",
    "        K += 10\n",
    "    \n",
    "    delta = auxiliary - point\n",
    "    tmp1 = np.array([[float(auxiliary[0])], [float(point[0])]])\n",
    "    tmp2 = np.array([[float(point[0])], [float(auxiliary[1])]])\n",
    "\n",
    "    z = informationGain(point, x_name, y_name, data)\n",
    "    z_delta_x = informationGain(tmp1, x_name, y_name, data)\n",
    "    z_delta_y = informationGain(tmp2, x_name, y_name, data)\n",
    "    z_delta = np.array([[z_delta_x], [z_delta_y]])\n",
    "\n",
    "    return (z_delta - z) / delta\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientIG(point, x_name, y_name, data):\n",
    "    _, auxiliary_idx = credit_kdtree.query(point.flatten(), k= 2)\n",
    "    auxiliary_idx = auxiliary_idx[-1]\n",
    "    auxiliary = train_set.loc[auxiliary_idx, [x_name, y_name]].to_numpy().reshape(2,1)\n",
    "    delta = auxiliary - point\n",
    "    z = informationGain(point, x_name, y_name, data)\n",
    "    z_delta = [informationGain(np.array([[auxiliary[[0]]], point[[1]]]), x_name, y_name, data), \\\n",
    "        informationGain(np.array([[point[[0]]], auxiliary[[1]]]), x_name, y_name, data)]\n",
    "    grad = (z_delta - z) / delta\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalEntropy(new_sam_0 , new_sam_1):\n",
    "    total = len(new_sam_1) + len(new_sam_0)\n",
    "    return Entropy(new_sam_0, SAR_type) * (len(new_sam_0) / total) \\\n",
    "            + Entropy(new_sam_1, SAR_type) * (len(new_sam_1) / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(point, x_name, y_name, data):\n",
    "    \n",
    "    idx1 = set(data[data[x_name] >= float(point[0])].index)\n",
    "    idx2 = set(data[data[y_name] >= float(point[1])].index)\n",
    "    \n",
    "    #idx3 = set(data[data[y_name] >= float(y_thre)].index)\n",
    "    \n",
    "    idx = list(idx1 & idx2)\n",
    "    new_sam_1 = data.iloc[idx]\n",
    "    new_sam_0 = data.drop(index= new_sam_1.index)\n",
    "    #print(len(new_sam_1), len(new_sam_0))\n",
    "    if len(set(new_sam_1.index) & set(new_sam_0.index)) != 0:\n",
    "        raise ValueError('new sam 0 and new sam 1 are overlapped')\n",
    "        \n",
    "    return -(father_entropy - conditionalEntropy(new_sam_0, new_sam_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(point, x_name, y_name, data):\n",
    "    idx1 = set(data[data[x_name] > float(point[0])].index)\n",
    "    idx2 = set(data[data[y_name] > float(point[1])].index)\n",
    "    idx = list(idx1 & idx2)\n",
    "    #print(f'idx:{len(idx)}')\n",
    "    new_sam_1 = data.iloc[idx]\n",
    "    new_sam_0 = data.drop(index= new_sam_1.index)\n",
    "    \n",
    "    if len(set(new_sam_1.index) & set(new_sam_0.index)) != 0:\n",
    "        raise ValueError('new sam 1 and new sam 0 are overlapped')\n",
    "    \n",
    "    TP = new_sam_1[new_sam_1[SAR_type] != 0].shape[0]\n",
    "    TN = new_sam_0[new_sam_0[SAR_type] == 0].shape[0]\n",
    "    FP = new_sam_1[new_sam_1[SAR_type] == 0].shape[0]\n",
    "    FN = new_sam_0[new_sam_0[SAR_type] != 0].shape[0]\n",
    "    \n",
    "    return -(TP / (TP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecallFilterRate(point, x_name, y_name, data):\n",
    "    idx1 = set(data[data[x_name] > float(point[0])].index)\n",
    "    idx2 = set(data[data[y_name] > float(point[1])].index)\n",
    "    \n",
    "    idx = list(idx1 & idx2)\n",
    "    \n",
    "    new_sam_1 = data.iloc[idx]\n",
    "    new_sam_0 = data.drop(index= new_sam_1.index)\n",
    "    \n",
    "    if len(set(new_sam_1.index) & set(new_sam_0.index)) != 0:\n",
    "        raise ValueError('new_sam_1 and new_sam_0 are voerlapped')\n",
    "        \n",
    "    TP = new_sam_1[new_sam_1[SAR_type] != 0].shape[0]\n",
    "    TN = new_sam_0[new_sam_0[SAR_type] == 0].shape[0]\n",
    "    FP = new_sam_1[new_sam_1[SAR_type] == 0].shape[0]\n",
    "    FN = new_sam_0[new_sam_0[SAR_type] != 0].shape[0]\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    filter_rate = (TN + FN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    return recall, filter_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    x = list(results.loc[results['iteration number'] % 50 == 0].index)\n",
    "    x.append(len(results) - 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, results.loc[x, 'information gain'], 'o--')\n",
    "    plt.plot(x, results.loc[x, 'recall'], 'o--')\n",
    "    plt.plot(x, results.loc[x, 'filter rate'], 'o--')\n",
    "    plt.xlabel('iteration number')\n",
    "\n",
    "    plt.legend(['information gain', 'recall', 'filter rate'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seperator(results):\n",
    "    x = list(results.loc[results['iteration number'] % 50 == 0].index)\n",
    "    x.append(len(results) - 1)\n",
    "\n",
    "    plot_name = ['information gain', 'recall', 'filter rate']\n",
    "    color = ['r', 'b', 'g']\n",
    "    plt.figure()\n",
    "    for i, name in enumerate(plot_name):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.plot(x, results.loc[x, name], 'o--', color= color[i])\n",
    "        plt.title(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientPenalty(point, upperbound=np.zeros((2,1))):\n",
    "    if np.any(point < 0): # any 是只要有一個是 True 就 output True，但 all 是要全部都為 True 才 output True\n",
    "        raise ValueError('point are negative!')\n",
    "\n",
    "    if np.linalg.norm(upperbound) == 0:\n",
    "        return 1 / point\n",
    "    else:\n",
    "        return 1 / (upperbound - point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientRecall(point, x_name, y_name, data):\n",
    "    x1 = float(point[0])\n",
    "    y1 = float(point[1])\n",
    "\n",
    "    if len(np.sort(np.unique(data.loc[data[x_name] > x1, x_name]), kind= 'stable')) > auxiliary_point_select_behind_x:\n",
    "        x2 = np.sort(np.unique(data.loc[data[x_name] > x1, x_name]), kind= 'stable')[auxiliary_point_select_behind_x]\n",
    "    else:\n",
    "        x2 = np.sort(np.unique(data.loc[data[x_name] < x1, x_name]), kind= 'stable')[-auxiliary_point_select_behind_x]\n",
    "    \n",
    "    if len(np.sort(np.unique(data.loc[data[y_name] > y1, y_name]), kind= 'stable')) > auxiliary_point_select_behind_y:\n",
    "        y2 = np.sort(np.unique(data.loc[data[y_name] > y1, y_name]), kind= 'stable')[auxiliary_point_select_behind_y]\n",
    "    else:\n",
    "        y2 = np.sort(np.unique(data.loc[data[y_name] < y1, y_name]), kind= 'stable')[-auxiliary_point_select_behind_y]\n",
    "    \n",
    "    delta_x = x2 - x1\n",
    "    delta_y = y2 - y1\n",
    "\n",
    "    z = Recall(point, x_name, y_name, data)\n",
    "    z_delta_x = Recall(np.array([[x2], [y1]]), x_name, y_name, data)\n",
    "    z_delta_y = Recall(np.array([[x1], [y2]]), x_name, y_name, data)\n",
    "\n",
    "    grad = np.array([[(z_delta_x - z) / delta_x], [(z_delta_y - z) / delta_y]])\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-(sigmoid_b + sigmoid_w * x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidDiff(x):\n",
    "    tmp = sigmoid(x)\n",
    "    return tmp * (1 - tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(point, IG, Recall):\n",
    "    x = float(point[0])\n",
    "    y = float(point[1])\n",
    "    loss = IG * sigmoid(Recall) + gamma * (log(x) + log(y) + log(Mx - x) + log(My - y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGSWithUpDownPenaltyRecall(point0, x_name, y_name, upperbound, data):\n",
    "    results = pd.DataFrame(columns= ['iteration number', 'x1', 'y1', 'new x1', 'new y1', 'information gain', 'recall', 'filter rate', 'loss'])\n",
    "    d = 2\n",
    "    gradIG = gradientIG(point0, x_name, y_name, data)\n",
    "    gradRecall = gradientRecall(point0, x_name, y_name, data)\n",
    "    gradientPenaltyDown = gradientPenalty(point0)\n",
    "    gradientPenaltyUp = gradientPenalty(point0, upperbound)\n",
    "    recall = Recall(point0, x_name, y_name, data)\n",
    "    information_gain = informationGain(point0, x_name, y_name, data)\n",
    "    grads = gradIG * sigmoid(recall) + information_gain * sigmoidDiff(recall) * gradRecall + gamma * (gradientPenaltyDown + gradientPenaltyUp)\n",
    "    print(f'recall:{recall}, sigmoid:{sigmoid(recall)}')\n",
    "    H = np.eye(d)\n",
    "    point = point0\n",
    "    n = 1\n",
    "\n",
    "    print(f'gradIG:\\n{gradIG}')\n",
    "    print(f'gradRecall:\\n{gradRecall}')\n",
    "    print(f'gradPenaltyDown:\\n{gradientPenaltyDown}')\n",
    "    print(f'gradPenaltyUp:\\n{gradientPenaltyUp}')\n",
    "    print(f'Hessian matrix:\\n{H}')\n",
    "\n",
    "\n",
    "    while np.linalg.norm(grads) > 1e-50:\n",
    "        if n > N_max:\n",
    "            print(f'maximum iteration reached!')\n",
    "            break\n",
    "        \n",
    "        update_direction = -H @ grads\n",
    "        print(f'update_direction:\\n{update_direction}')\n",
    "        new_point = point + step_size * update_direction\n",
    "\n",
    "        information_gain = informationGain(new_point, x_name, y_name, data)\n",
    "        recall, filter_rate = RecallFilterRate(new_point, x_name, y_name, data)\n",
    "        loss = loss_func(point, information_gain, recall)\n",
    "\n",
    "        print(f'n={n}, new x1:{float(new_point[0])}, new y1: {float(new_point[1])},\\ninformation gain: {information_gain}, recall:{recall}, filter rate:{filter_rate}')\n",
    "\n",
    "        tmp_results = pd.DataFrame({'iteration number': n, 'x1': float(point[0]), 'y1': float(point[1]), 'new x1': float(new_point[0]), \\\n",
    "                                 'new y1': float(new_point[1]), 'information gain': information_gain, 'recall': recall, \\\n",
    "                                 'filter rate': filter_rate, 'loss': loss}, index= [n])\n",
    "        results = pd.concat([results, tmp_results])\n",
    "\n",
    "        new_gradIG = gradientIG(new_point, x_name, y_name, data)\n",
    "        new_gradRecall = gradientRecall(new_point, x_name, y_name, data)\n",
    "        new_gradPenaltyDown = gradientPenalty(new_point)\n",
    "        new_gradPenaltyUp = gradientPenalty(new_point, upperbound)\n",
    "        new_grads = new_gradIG * sigmoid(recall) + information_gain * sigmoidDiff(recall) * new_gradRecall + gamma * (new_gradPenaltyDown + new_gradPenaltyUp)\n",
    "\n",
    "        y = new_grads - grads\n",
    "        s = new_point - point\n",
    "        denominator = 1 / (y.T @ s)\n",
    "        li = np.eye(d) - (denominator * (s @ y.T))\n",
    "        ri = np.eye(d) - (denominator * (y @ s.T))\n",
    "        H = li @ H @ ri + (denominator * (s @ s.T))\n",
    "\n",
    "        grads = new_grads\n",
    "        point = new_point\n",
    "        print('*'*32)\n",
    "        print(f'new_gradIG:\\n{new_gradIG}')\n",
    "        print(f'new_gradRecall:\\n{new_gradRecall}')\n",
    "        print(f'new_gradPenaltyDown:\\n{new_gradPenaltyDown}')\n",
    "        print(f'new_gradPenaltyUp:\\n{new_gradPenaltyUp}')\n",
    "        print(f'new_Hessian matrix:\\n{H}')\n",
    "        print(f'recall:{recall}, sigmoid:{sigmoid(recall)}')\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "    if np.linalg.norm(grads) < 1e-50:\n",
    "        print('Early stopping')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the higher recall in the Credit_Amt and number_of_Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = train_set[['Credit_Amt', 'number_of_Credit']]\n",
    "credit_kdtree = cKDTree(point, leafsize= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (SAR.loc[:, 'Credit_Amt'].to_numpy()**2 + SAR.loc[:, 'number_of_Credit'].to_numpy())**(1/2)\n",
    "SAR.insert(0,'SAR_Credit_distance', tmp)\n",
    "tmp= (SAR.loc[:, 'Debit_Amt'].to_numpy()**2 + SAR.loc[:, 'number_of_Debit'].to_numpy())**(1/2)\n",
    "SAR.insert(0, 'SAR_Debit_distance', tmp)\n",
    "\n",
    "del tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 235\n"
     ]
    }
   ],
   "source": [
    "idx_SAR_Credit_distance = SAR.loc[SAR['SAR_Credit_distance']!=0].sort_values(by= 'SAR_Credit_distance').index\n",
    "idx_SAR_Debit_distance = SAR.loc[SAR['SAR_Debit_distance']!=0].sort_values(by= 'SAR_Debit_distance').index\n",
    "print(len(idx_SAR_Credit_distance), len(idx_SAR_Debit_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_distance_results = pd.DataFrame(columns= ['Credit_Amt', 'number_of_Credit', 'recall', 'filterRate'])\n",
    "\n",
    "for idx in idx_SAR_Credit_distance:\n",
    "    x, y = SAR.loc[idx][['Credit_Amt', 'number_of_Credit']]\n",
    "    recall_tmp, filterRate_tmp= RecallFilterRate(np.array([[x], [y]]), 'Credit_Amt', 'number_of_Credit', train_set)\n",
    "    tmp = pd.DataFrame({'Credit_Amt':[x], 'number_of_Credit':[y], 'recall': [recall_tmp], 'filterRate': [filterRate_tmp]})\n",
    "    credit_distance_results = pd.concat([credit_distance_results, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Credit_Amt          4000.0\n",
       "number_of_Credit       1.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point = credit_distance_results.sort_values(by= 'recall', ascending= False).iloc[0,0:2]\n",
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point = point.to_numpy()\n",
    "point = point.reshape(2,1)\n",
    "point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.e+03],\n",
       "       [1.e+00]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter\n",
    "SAR_type = 'TO_SAR'\n",
    "eps = 1e-1000\n",
    "N_max = 1000\n",
    "x_name = 'Credit_Amt'\n",
    "y_name = 'number_of_Credit'\n",
    "step_size = 0.001\n",
    "gamma = 1e-5\n",
    "auxiliary_point_select_behind_x = 3\n",
    "auxiliary_point_select_behind_y = 3\n",
    "upperbound = np.array([[train_set[x_name].max()], [train_set[y_name].max()]])\n",
    "sigmoid_b = 0.8\n",
    "sigmoid_w = 5\n",
    "Mx = train_set[x_name].max()\n",
    "My = train_set[y_name].max()\n",
    "father_entropy = Entropy(train_set, SAR_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [[4.e+03]\n",
      " [1.e+00]]\n",
      "K= 2 aux [[4000.0]\n",
      " [1.0]]\n",
      "K= 12 aux [[4000.0]\n",
      " [1.0]]\n",
      "K= 22 aux [[4000.0]\n",
      " [1.0]]\n",
      "K= 32 aux [[4050.0]\n",
      " [1.0]]\n",
      "K= 42 aux [[3617.0]\n",
      " [2.0]]\n",
      "[[-2.057901597944917e-06]\n",
      " [0.00013225031909248897]]\n"
     ]
    }
   ],
   "source": [
    "print('point',point)\n",
    "data = train_set\n",
    "K = 2\n",
    "\n",
    "while True:\n",
    "    _, auxiliary_idx = credit_kdtree.query(point.flatten(), k= K)\n",
    "    auxiliary_idx  = auxiliary_idx[-1]\n",
    "    auxiliary = train_set.loc[auxiliary_idx, [x_name, y_name]].to_numpy().reshape(2, 1)\n",
    "    print('K=',K,'aux',auxiliary)\n",
    "    if auxiliary.all() != point.all():\n",
    "        break\n",
    "    K += 10\n",
    "# _, auxiliary_idx = credit_kdtree.query(point.flatten(), k= 100)\n",
    "# auxiliary_idx  =auxiliary_idx[-1]\n",
    "# auxiliary = train_set.loc[auxiliary_idx, [x_name, y_name]].to_numpy().reshape((2,1))\n",
    "\n",
    "delta = auxiliary - point\n",
    "tmp1 = np.array([[float(auxiliary[0])], [float(point[0])]])\n",
    "tmp2 = np.array([[float(point[0])], [float(auxiliary[1])]])\n",
    "z = informationGain(point, x_name, y_name, data)\n",
    "# #z_delta_x  = informationGain(np.array([[float(auxiliary[0])], float(point[1])]), x_name, y_name, data)\n",
    "# #z_delta_y = informationGain(np.array([[float(point[0])], float(auxiliary[1])]), x_name, y_name, data)\n",
    "# # z_delta = [informationGain(np.array([[float(auxiliary[0])], float(point[1])]), x_name, y_name, data), \\\n",
    "# #         informationGain(np.array([[float(point[0])], float(auxiliary[1])]), x_name, y_name, data)]\n",
    "z_delta_x = informationGain(tmp1, x_name, y_name, data)\n",
    "z_delta_y = informationGain(tmp2, x_name, y_name, data)\n",
    "z_delta = np.array([[z_delta_x], [z_delta_y]])\n",
    "grad = (z_delta - z) / delta\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.e+03]\n",
      " [1.e+00]]\n",
      "recall:-0.37453183520599254, sigmoid:0.2548977114266221\n",
      "gradIG:\n",
      "[[-2.057901597944917e-06]\n",
      " [0.00013225031909248897]]\n",
      "gradRecall:\n",
      "[[0.        ]\n",
      " [0.02434457]]\n",
      "gradPenaltyDown:\n",
      "[[2.5e-04]\n",
      " [1.0e+00]]\n",
      "gradPenaltyUp:\n",
      "[[4.30839491e-08]\n",
      " [1.17508813e-03]]\n",
      "Hessian matrix:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "update_direction:\n",
      "[[5.220539768178572e-07]\n",
      " [-4.007781173011453e-05]]\n",
      "n=1, new x1:4000.000000000522, new y1: 0.9999999599221883,\n",
      "information gain: -0.0008364830183348743, recall:0.40074906367041196, filter rate:0.36519400691509796\n",
      "********************************\n",
      "new_gradIG:\n",
      "[[-1602304.5384953993]\n",
      " [0.0]]\n",
      "new_gradRecall:\n",
      "[[0.        ]\n",
      " [0.02996255]]\n",
      "new_gradPenaltyDown:\n",
      "[[0.00024999999999996737]\n",
      " [1.0000000400778133]]\n",
      "new_gradPenaltyUp:\n",
      "[[4.3083949074772195e-08]\n",
      " [0.00117508813155453]]\n",
      "new_Hessian matrix:\n",
      "[[-3.4300160195389024e-16 -1.2257644879474482e-07]\n",
      " [-1.2257644879474482e-07 5894.643623701278]]\n",
      "recall:0.40074906367041196, sigmoid:0.9428778788088424\n",
      "update_direction:\n",
      "[[-5.171373629795541e-10]\n",
      " [-0.23624437733446862]]\n",
      "n=2, new x1:4000.0000000005216, new y1: 0.9997637155448538,\n",
      "information gain: -0.0008364830183348743, recall:0.40074906367041196, filter rate:0.36519400691509796\n",
      "********************************\n",
      "new_gradIG:\n",
      "[[-1603701.491013704]\n",
      " [0.0]]\n",
      "new_gradRecall:\n",
      "[[0.        ]\n",
      " [0.02996019]]\n",
      "new_gradPenaltyDown:\n",
      "[[0.0002499999999999674]\n",
      " [1.000236340298685]]\n",
      "new_gradPenaltyUp:\n",
      "[[4.3083949074772195e-08]\n",
      " [0.0011750878053407969]]\n",
      "new_Hessian matrix:\n",
      "[[3.456069778205193e-16 1.906720306378788e-07]\n",
      " [1.9067203063787884e-07 6034.266326064181]]\n",
      "recall:0.40074906367041196, sigmoid:0.9428778788088424\n",
      "update_direction:\n",
      "[[5.209384185126901e-10]\n",
      " [0.23603122939983223]]\n",
      "n=3, new x1:4000.000000000522, new y1: 0.9999997467742536,\n",
      "information gain: -0.0008364830183348743, recall:0.40074906367041196, filter rate:0.36519400691509796\n",
      "********************************\n",
      "new_gradIG:\n",
      "[[-1602304.5384953993]\n",
      " [0.0]]\n",
      "new_gradRecall:\n",
      "[[0.        ]\n",
      " [0.02996254]]\n",
      "new_gradPenaltyDown:\n",
      "[[0.00024999999999996737]\n",
      " [1.0000002532258105]]\n",
      "new_gradPenaltyUp:\n",
      "[[4.3083949074772195e-08]\n",
      " [0.0011750881312602086]]\n",
      "new_Hessian matrix:\n",
      "[[3.45606332542308e-16 1.9049966713016416e-07]\n",
      " [1.9049966713016416e-07 6034.087573425878]]\n",
      "recall:0.40074906367041196, sigmoid:0.9428778788088424\n",
      "update_direction:\n",
      "[[5.204841888559537e-10]\n",
      " [0.2355361170043045]]\n",
      "n=4, new x1:4000.0000000005225, new y1: 1.000235282891258,\n",
      "information gain: -0.0006555575926249224, recall:0.37453183520599254, filter rate:0.41665386092969653\n",
      "********************************\n",
      "new_gradIG:\n",
      "[[-1254644.379056474]\n",
      " [0.7689697484701031]]\n",
      "new_gradRecall:\n",
      "[[0.      ]\n",
      " [0.024346]]\n",
      "new_gradPenaltyDown:\n",
      "[[0.0002499999999999673]\n",
      " [0.9997647724537593]]\n",
      "new_gradPenaltyUp:\n",
      "[[4.3083949074772195e-08]\n",
      " [0.0011750884564961333]]\n",
      "new_Hessian matrix:\n",
      "[[2.2062520468364008e-14 -1.0341897899224356e-08]\n",
      " [-1.0341897899224354e-08 0.005175560253090302]]\n",
      "recall:0.37453183520599254, sigmoid:0.9353939172599599\n",
      "update_direction:\n",
      "[[3.333119444819058e-08]\n",
      " [-0.015859887646639922]]\n",
      "n=5, new x1:4000.0000000005557, new y1: 1.0002194230036112,\n",
      "information gain: -0.0006555575926249224, recall:0.37453183520599254, filter rate:0.41665386092969653\n",
      "********************************\n",
      "new_gradIG:\n",
      "[[-1179694.2647593196]\n",
      " [0.8245508571677458]]\n",
      "new_gradRecall:\n",
      "[[0.       ]\n",
      " [0.0243459]]\n",
      "new_gradPenaltyDown:\n",
      "[[0.00024999999999996526]\n",
      " [0.9997806251322812]]\n",
      "new_gradPenaltyUp:\n",
      "[[4.3083949074772195e-08]\n",
      " [0.0011750884345962795]]\n",
      "new_Hessian matrix:\n",
      "[[1.1679725676778616e-15 -9.364744896353582e-10]\n",
      " [-9.364744896353555e-10 0.0009577636671142221]]\n",
      "recall:0.37453183520599254, sigmoid:0.9353939172599599\n",
      "update_direction:\n",
      "[[2.0111253937513627e-09]\n",
      " [-0.001772092269558262]]\n",
      "n=6, new x1:4000.0000000005575, new y1: 1.0002176509113416,\n",
      "information gain: -0.0006555575926249224, recall:0.37453183520599254, filter rate:0.41665386092969653\n",
      "********************************\n",
      "new_gradIG:\n",
      "[[-1175845.3438302518]\n",
      " [0.8312642689833243]]\n",
      "new_gradRecall:\n",
      "[[0.        ]\n",
      " [0.02434589]]\n",
      "new_gradPenaltyDown:\n",
      "[[0.00024999999999996515]\n",
      " [0.9997823964502692]]\n",
      "new_gradPenaltyUp:\n",
      "[[4.3083949074772195e-08]\n",
      " [0.0011750884321493165]]\n",
      "new_Hessian matrix:\n",
      "[[7.808638697494135e-16 -1.5802089585541518e-10]\n",
      " [-1.5802089585541476e-10 -0.0001915982211323511]]\n",
      "recall:0.37453183520599254, sigmoid:0.9353939172599599\n",
      "update_direction:\n",
      "[[9.817275305307927e-10]\n",
      " [-2.482304103128242e-05]]\n",
      "n=7, new x1:4000.0000000005584, new y1: 1.0002176260883007,\n",
      "information gain: -0.0006555575926249224, recall:0.37453183520599254, filter rate:0.41665386092969653\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(1111)\n",
    "    #x1 = intialPoint('Credit_Amt', new_train_set)\n",
    "    #y1 = intialPoint('number_of_Credit', new_train_set)\n",
    "    #point = np.array([[x1], [y1]])\n",
    "    print(point)\n",
    "    results = BFGSWithUpDownPenaltyRecall(point, x_name, y_name, upperbound, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration number</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>new x1</th>\n",
       "      <th>new y1</th>\n",
       "      <th>information gain</th>\n",
       "      <th>recall</th>\n",
       "      <th>filter rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025028</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025051</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.416654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025051</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025075</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.416654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025075</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025098</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.416654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025098</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025121</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.416654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025121</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.025145</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.416654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iteration number      x1        y1  new x1    new y1  information gain  \\\n",
       "995             996.0  4000.0  1.025028  4000.0  1.025051         -0.000656   \n",
       "996             997.0  4000.0  1.025051  4000.0  1.025075         -0.000656   \n",
       "997             998.0  4000.0  1.025075  4000.0  1.025098         -0.000656   \n",
       "998             999.0  4000.0  1.025098  4000.0  1.025121         -0.000656   \n",
       "999            1000.0  4000.0  1.025121  4000.0  1.025145         -0.000656   \n",
       "\n",
       "       recall  filter rate  \n",
       "995  0.374532     0.416654  \n",
       "996  0.374532     0.416654  \n",
       "997  0.374532     0.416654  \n",
       "998  0.374532     0.416654  \n",
       "999  0.374532     0.416654  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('AntiMoneyLaundering')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37457cf4a8118c030724fb1e937b4d8b25b52507af80f55da5435ced93987267"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
